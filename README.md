This repository is the official implementation of "Enhancing Generalization in Neural Networks: Innovations in Dropout Techniques with Sector-wise and LRP-guided Approaches" paper.
The Sector-wise dropout approach systematically divides the nodes in each layer into sectors, applying deactivation based on a fixed dropout probability to ensure a more structured network architecture. LRP-guided Dropout leverages scores from Layer-wise Relevance Propagation (LRP), computed subsequent to parameter updates, to selectively deactivate nodes positioned within the highest percentile of LRP scores. Through empirical evaluation, we establish that these innovative approaches markedly enhance the balance and generalization capacity of neural networks, thus representing a significant progression in mitigating the overfitting conundrum.